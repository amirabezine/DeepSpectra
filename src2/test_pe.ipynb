{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1246818a-972f-498b-84f8-d3bdc61c2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pe import PositionalEncoding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16f22a4-1ccf-4f27-b8ec-0dea5afa6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config\n",
    "config = {\n",
    "    'model': {\n",
    "        'pe_dim': 10,  # Example value for pe_dim, replace with your actual value\n",
    "        'omega': 1.0,  # Example omega value, replace with your actual value\n",
    "        'sigma': 1.0,  # Example sigma value, replace with your actual value\n",
    "        'pe_bias': True,  # Example bias setting, replace with your actual value\n",
    "        'seed': 42  # Example seed value, replace with your actual value\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bfdb79a-1939-479b-b1f4-08e5b534bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract positional encoding arguments from config\n",
    "pe_args = (\n",
    "    1,  # Dimension of the input (wavelengths)\n",
    "    config['model']['pe_dim'],\n",
    "    config['model']['omega'],\n",
    "    config['model']['sigma'],\n",
    "    config['model']['pe_bias'],\n",
    "    config['model']['seed']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148abb9c-86d6-4d9d-a433-b5fa185047e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wavelength_grid():\n",
    "    grid = [\n",
    "        (15050, 15850, 0.2),\n",
    "        (15870, 16440, 0.2),\n",
    "        (16475, 17005, 0.2),\n",
    "        (4700, 4930, 0.05),\n",
    "        (5650, 5880, 0.05),\n",
    "        (6420, 6800, 0.05),\n",
    "        (7500, 7920, 0.05)\n",
    "    ]\n",
    "\n",
    "    wavelength_grid = []\n",
    "    for start, end, step in grid:\n",
    "        wavelength_grid.extend(np.arange(start, end + step, step))\n",
    "    \n",
    "    return np.array(wavelength_grid)\n",
    "\n",
    "def normalize_wavelengths(wavelengths, max_wavelength):\n",
    "    return wavelengths / max_wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd45f21-e0c2-4169-ae18-35ef916e9e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34714, 10])\n"
     ]
    }
   ],
   "source": [
    "# Generate wavelength grid\n",
    "wavelength_grid = generate_wavelength_grid()\n",
    "\n",
    "# Normalize the wavelength grid\n",
    "max_wavelength = 17100\n",
    "normalized_wavelength_grid = normalize_wavelengths(wavelength_grid, max_wavelength)\n",
    "\n",
    "# Initialize Positional Encoding with the arguments from config\n",
    "positional_encoding = PositionalEncoding(pe_args)\n",
    "\n",
    "# Generate positional encoding tensor\n",
    "wavelength_grid_tensor = torch.tensor(normalized_wavelength_grid, dtype=torch.float16)\n",
    "pe_tensor = positional_encoding(wavelength_grid_tensor)\n",
    "\n",
    "print(pe_tensor.shape)  # Verify the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411b00ea-0a4b-42a9-908c-8ed6389475bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4727,  0.9746, -0.3076,  ...,  0.9517,  0.8340, -0.6118],\n",
      "        [-0.4736,  0.9746, -0.3083,  ...,  0.9512,  0.8340, -0.6143],\n",
      "        [-0.4736,  0.9746, -0.3083,  ...,  0.9512,  0.8340, -0.6143],\n",
      "        ...,\n",
      "        [ 0.3794,  0.9937,  0.2966,  ...,  0.9551,  1.0000,  0.7573],\n",
      "        [ 0.3794,  0.9937,  0.2966,  ...,  0.9551,  1.0000,  0.7573],\n",
      "        [ 0.3794,  0.9937,  0.2966,  ...,  0.9551,  1.0000,  0.7573]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(pe_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a101c714-1d82-4a22-9540-12979ac7a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15050.   15050.2  15050.4  ...  7919.95  7920.    7920.05]\n"
     ]
    }
   ],
   "source": [
    "print (wavelength_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60529ea-1882-46c8-ae2f-4a35833fde6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains NaNs: False\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the pe_tensor\n",
    "contains_nan = torch.isnan(pe_tensor).any()\n",
    "print(f\"Contains NaNs: {contains_nan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a230d8-0c73-42a1-a2e6-15607f249a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import IterableSpectraDataset, collate_fn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6491eaf-3010-4205-bbae-e24c6681b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_dir = '../data/healpixfiles_inter'\n",
    "dataset = IterableSpectraDataset(hdf5_dir, n_samples_per_spectrum=1000, n_subspectra=5, yield_full_spectrum=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f13f92ca-72c0-4dfc-b71a-df66e33033da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wavelength shape: torch.Size([32, 25300])\n",
      "Flux shape: torch.Size([32, 25300])\n",
      "Spectrum IDs: ('139_galah_337', '100_apogee_118', '100_apogee_133', '100_apogee_141', '100_apogee_289', '63_apogee_121', '26_apogee_178', '26_apogee_275', '50_apogee_147', '50_apogee_292', '50_apogee_298', '176_apogee_233', '176_galah_377', '30_apogee_107', '30_apogee_209', '30_apogee_213', '65_apogee_123', '11_apogee_285', '149_apogee_172', '149_apogee_184', '72_galah_376', '107_galah_380', '140_galah_348', '111_galah_340', '146_galah_356', '13_apogee_146', '13_apogee_151', '13_apogee_167', '13_apogee_177', '13_apogee_255', '13_apogee_263', '78_apogee_110')\n",
      "Lengths: tensor([20747,  9495,  9495,  9495,  9495,  9495,  9495,  9495,  9495,  9495,\n",
      "         9495,  9495, 20762,  9495,  9495,  9495,  9495,  9495,  9495,  9495,\n",
      "        20780, 20765, 14376, 20767, 20752,  9495,  9495,  9495,  9495,  9495,\n",
      "         9495,  9495])\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# Print shapes of wavelength and flux tensors, and the spectrum IDs\n",
    "print(\"Wavelength shape:\", first_batch['wavelength'].shape)\n",
    "print(\"Flux shape:\", first_batch['flux'].shape)\n",
    "print(\"Spectrum IDs:\", first_batch['spectrum_id'])\n",
    "print(\"Lengths:\", first_batch['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df12f2f3-e9ff-443b-94f6-e9ef0d4df448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Generator, DownsamplingLayer, FullNetwork\n",
    "from pe import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46803d46-fddb-4935-bac8-76149386f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "latent_dim = 5  # Example latent dimension\n",
    "output_dim = 25300  # Example output dimension (max length of wavelengths)\n",
    "layers = [512, 512]  # Example hidden layers\n",
    "activation_function = 'LeakyReLU'  # Example activation function\n",
    "\n",
    "# Define Generator\n",
    "generator = Generator(latent_dim, output_dim, layers, activation_function)\n",
    "\n",
    "# Define downsampling layer\n",
    "downsampling_layer = DownsamplingLayer()\n",
    "\n",
    "# Create the full network\n",
    "full_network = FullNetwork(generator, downsampling_layer)\n",
    "\n",
    "# Define Positional Encoding\n",
    "pe_args = (\n",
    "    1,  # Dimension of the input (wavelengths)\n",
    "    128,  # pe_dim example value\n",
    "    1.0,  # omega example value\n",
    "    1.0,  # sigma example value\n",
    "    True,  # pe_bias example value\n",
    "    42  # seed example value\n",
    ")\n",
    "\n",
    "positional_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e86b7a-b0b6-48ca-9f9a-a692a14fae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalEncoding(\n",
      "  (rand_gaus): RandGaus(\n",
      "    (mappings): ModuleList(\n",
      "      (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "positional_encoding = PositionalEncoding(pe_args)\n",
    "print(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0679b653-109a-4f52-9dd9-c2bc3ed7138b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m positional_encoding_ref \u001b[38;5;241m=\u001b[39m positional_encoding(wavelength_grid)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m generated_flux \u001b[38;5;241m=\u001b[39m \u001b[43mfull_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositional_encoding_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved_wavelength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the shape of the generated flux\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated flux shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_flux\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/deepSpectra/src2/model.py:52\u001b[0m, in \u001b[0;36mFullNetwork.forward\u001b[0;34m(self, latent_z, positional_encoding, observed_wavelength)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Expand the latent vector to match the positional encoding dimensions\u001b[39;00m\n\u001b[1;32m     51\u001b[0m latent_z_expanded \u001b[38;5;241m=\u001b[39m latent_z\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, positional_encoding_ref\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m input_to_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_z_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositional_encoding_ref\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Generate high-resolution intermediate spectrum\u001b[39;00m\n\u001b[1;32m     55\u001b[0m high_res_flux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(input_to_generator\u001b[38;5;241m.\u001b[39mview(input_to_generator\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(input_to_generator\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate positional encoding tensor\n",
    "wavelength_grid = torch.tensor(generate_wavelength_grid(), dtype=torch.float32)\n",
    "wavelength_grid = normalize_wavelengths(wavelength_grid, 17100)  # Normalize the wavelength grid\n",
    "\n",
    "# Example input tensors\n",
    "batch_size = 5\n",
    "latent_z = torch.randn((batch_size, latent_dim))  # Example latent space vector\n",
    "observed_wavelength = torch.randn((batch_size, positional_dim))  # Example observed wavelengths\n",
    "\n",
    "# Use positional encoding by reference\n",
    "positional_encoding_ref = positional_encoding(wavelength_grid)\n",
    "\n",
    "# Forward pass\n",
    "generated_flux = full_network(latent_z, positional_encoding_ref, observed_wavelength)\n",
    "\n",
    "# Print the shape of the generated flux\n",
    "print(f\"Generated flux shape: {generated_flux.shape}\")\n",
    "\n",
    "# Define the weighted MSE loss function\n",
    "def weighted_mse_loss(input, target, weight):\n",
    "    assert input.shape == target.shape == weight.shape, f'Shapes of input {input.shape}, target {target.shape}, and weight {weight.shape} must match'\n",
    "    loss = torch.mean(weight * (input - target) ** 2) \n",
    "    return loss\n",
    "\n",
    "# Example target and weight tensors\n",
    "sampled_flux = torch.randn((batch_size, positional_dim))  # Example sampled flux (real flux)\n",
    "weights = torch.ones((batch_size, positional_dim))  # Example weights\n",
    "\n",
    "# Compute the weighted MSE loss\n",
    "loss = weighted_mse_loss(generated_flux, sampled_flux, weights)\n",
    "\n",
    "# Backpropagation\n",
    "optimizer = torch.optim.Adam(full_network.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Print total loss\n",
    "print(f\"Total loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815fe2c-e54b-4d3b-888f-8166bee2c5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
